{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import lib(s)","metadata":{}},{"cell_type":"code","source":"!pip install ecos\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from abc import abstractmethod\nimport os\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\nimport numpy as np\n\nimport copy\nimport cvxpy as cp\n\nimport argparse\n\nimport time\n\nimport glob\n\nfrom torch.utils import data\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.models as models\nimport matplotlib.pyplot as plt\n\nimport ecos\n\nimport math\nfrom scipy.linalg import sqrtm\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self, name, args=True):\n        super(Model, self).__init__()\n        self.name = name\n\n        if self.name == \"linear\":\n            [self.n_dim, self.n_out] = args\n            self.fc = nn.Linear(self.n_dim, self.n_out)\n\n        elif self.name == \"mnist\":\n            self.n_cls = 10\n            self.fc1 = nn.Linear(1 * 28 * 28, 200)\n            self.fc2 = nn.Linear(200, 200)\n            self.fc3 = nn.Linear(200, self.n_cls)\n\n        elif self.name == \"emnist\":\n            self.n_cls = 10\n            self.fc1 = nn.Linear(1 * 28 * 28, 100)\n            self.fc2 = nn.Linear(100, 100)\n            self.fc3 = nn.Linear(100, self.n_cls)\n\n        elif self.name == \"cifar10\":\n            self.n_cls = 10\n            self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=5)\n            self.conv2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=5)\n            self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n            self.fc1 = nn.Linear(64 * 5 * 5, 384)\n            self.fc2 = nn.Linear(384, 192)\n            self.fc3 = nn.Linear(192, self.n_cls)\n\n        elif self.name == \"cifar100\":\n            self.n_cls = 100\n            self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=5)\n            self.conv2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=5)\n            self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n            self.fc1 = nn.Linear(64 * 5 * 5, 384)\n            self.fc2 = nn.Linear(384, 192)\n            self.fc3 = nn.Linear(192, self.n_cls)\n\n        elif self.name == \"resnet18\":\n            resnet18 = models.resnet18()\n            resnet18.fc = nn.Linear(512, 10)\n\n            # Change BN to GN\n            resnet18.bn1 = nn.GroupNorm(num_groups=2, num_channels=64)\n\n            resnet18.layer1[0].bn1 = nn.GroupNorm(num_groups=2, num_channels=64)\n            resnet18.layer1[0].bn2 = nn.GroupNorm(num_groups=2, num_channels=64)\n            resnet18.layer1[1].bn1 = nn.GroupNorm(num_groups=2, num_channels=64)\n            resnet18.layer1[1].bn2 = nn.GroupNorm(num_groups=2, num_channels=64)\n\n            resnet18.layer2[0].bn1 = nn.GroupNorm(num_groups=2, num_channels=128)\n            resnet18.layer2[0].bn2 = nn.GroupNorm(num_groups=2, num_channels=128)\n            resnet18.layer2[0].downsample[1] = nn.GroupNorm(\n                num_groups=2, num_channels=128\n            )\n            resnet18.layer2[1].bn1 = nn.GroupNorm(num_groups=2, num_channels=128)\n            resnet18.layer2[1].bn2 = nn.GroupNorm(num_groups=2, num_channels=128)\n\n            resnet18.layer3[0].bn1 = nn.GroupNorm(num_groups=2, num_channels=256)\n            resnet18.layer3[0].bn2 = nn.GroupNorm(num_groups=2, num_channels=256)\n            resnet18.layer3[0].downsample[1] = nn.GroupNorm(\n                num_groups=2, num_channels=256\n            )\n            resnet18.layer3[1].bn1 = nn.GroupNorm(num_groups=2, num_channels=256)\n            resnet18.layer3[1].bn2 = nn.GroupNorm(num_groups=2, num_channels=256)\n\n            resnet18.layer4[0].bn1 = nn.GroupNorm(num_groups=2, num_channels=512)\n            resnet18.layer4[0].bn2 = nn.GroupNorm(num_groups=2, num_channels=512)\n            resnet18.layer4[0].downsample[1] = nn.GroupNorm(\n                num_groups=2, num_channels=512\n            )\n            resnet18.layer4[1].bn1 = nn.GroupNorm(num_groups=2, num_channels=512)\n            resnet18.layer4[1].bn2 = nn.GroupNorm(num_groups=2, num_channels=512)\n\n            assert len(dict(resnet18.named_parameters()).keys()) == len(\n                resnet18.state_dict().keys()\n            ), \"More BN layers are there...\"\n            self.model = resnet18\n\n    def forward(self, x):\n        if self.name == \"linear\":\n            x = self.fc(x)\n\n        elif self.name == \"mnist\":\n            x = x.view(-1, 1 * 28 * 28)\n            x = F.relu(self.fc1(x))\n            x = F.relu(self.fc2(x))\n            x = self.fc3(x)\n\n        elif self.name == \"emnist\":\n            x = x.view(-1, 1 * 28 * 28)\n            x = F.relu(self.fc1(x))\n            x = F.relu(self.fc2(x))\n            x = self.fc3(x)\n\n        elif self.name == \"cifar10\":\n            x = self.pool(F.relu(self.conv1(x)))\n            x = self.pool(F.relu(self.conv2(x)))\n            x = x.view(-1, 64 * 5 * 5)\n            x = F.relu(self.fc1(x))\n            x = F.relu(self.fc2(x))\n            x = self.fc3(x)\n\n        elif self.name == \"cifar100\":\n            x = self.pool(F.relu(self.conv1(x)))\n            x = self.pool(F.relu(self.conv2(x)))\n            x = x.view(-1, 64 * 5 * 5)\n            x = F.relu(self.fc1(x))\n            x = F.relu(self.fc2(x))\n            x = self.fc3(x)\n\n        elif self.name == \"resnet18\":\n            x = self.model(x)\n\n        return x\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Args","metadata":{}},{"cell_type":"code","source":"def args_parser():\n    parser = argparse.ArgumentParser()\n\n    # FedDyn setup\n    parser.add_argument(\n        \"--algorithm_name\", type=str, default=\"FedDyn\", help=\"algorithm name\"\n    )\n\n    parser.add_argument(\"--n_clients\", type=int, default=30, help=\"number of clients\")\n\n    parser.add_argument(\n        \"--comm_rounds\", type=int, default=50, help=\"number of communication rounds\"\n    )\n\n    parser.add_argument(\"--dataset_name\", type=str, default=\"cifar10\")\n\n\n    parser.add_argument(\"--lr\", type=float, default=0.03, help=\"learning rate\")\n\n    parser.add_argument(\n        \"--act_prob\", type=float, default=0.9, help=\"probability of active clients\"\n    )\n\n    parser.add_argument(\n        \"--lr_decay_per_round\",\n        type=float,\n        default=0.99,\n        help=\"learning rate decay per round\",\n    )\n\n    parser.add_argument(\"--batch_size\", type=int, default=50, help=\"batch size\")\n\n    parser.add_argument(\n        \"--epoch\", type=int, default=5, help=\"local epoch for client training\"\n    )\n\n    parser.add_argument(\"--weight_decay\", type=float, default=1e-2, help=\"weight decay\")\n\n    parser.add_argument(\n        \"--max_norm\", type=float, default=10, help=\"max norm for gradient clipping\"\n    )\n\n    parser.add_argument(\"--model_name\", type=str, default=\"cifar10\", help=\"model name\")\n\n    parser.add_argument(\n        \"--rule\", type=str, default=\"iid\", help=\"the rule of data partitioning\"\n    )\n\n    parser.add_argument(\"--rand_seed\", type=int, default=1, help=\"random seed\")\n\n    parser.add_argument(\"--save_period\", type=int, default=1, help=\"save period\")\n\n    parser.add_argument(\"--print_per\", type=int, default=5, help=\"print period\")\n\n    # RIS FL setup\n    parser.add_argument(\n        \"--n_RIS_ele\", type=int, default=40, help=\"number of RIS elements\"\n    )\n\n    parser.add_argument(\n        \"--n_receive_ant\", type=int, default=5, help=\"number of receive antennas\"\n    )\n\n    parser.add_argument(\n        \"--alpha_direct\", type=float, default=3.76, help=\"path loss component\"\n    )\n\n    parser.add_argument(\n        \"--SNR\", type=float, default=90.0, help=\"noise variance/0.1W in dB\"\n    )\n\n    parser.add_argument(\n        \"--location_range\",\n        type=int,\n        default=30,\n        help=\"location range between clients and RIS\",\n    )\n\n    parser.add_argument(\n        \"--Jmax\", type=int, default=50, help=\"number of maximum Gibbs Outer loops\"\n    )\n\n    parser.add_argument(\n        \"--tau\", type=float, default=0.03, help=\"tau, the SCA regularization term\"\n    )\n\n    parser.add_argument(\n        \"--nit\", type=int, default=100, help=\"I_max, number of maximum SCA loops\"\n    )\n\n    parser.add_argument(\n        \"--threshold\",\n        type=float,\n        default=1e-2,\n        help=\"epsilon, SCA early stopping criteria\",\n    )\n\n    parser.add_argument(\n        \"--transmit_power\", type=float, default=0.003, help=\"transmit power\"\n    )\n\n    parser.add_argument(\n        \"--noiseless\", type=bool, default=False, help=\"whether the channel is noiseless\"\n    )\n\n    args = parser.parse_args(args=[])\n\n    return args\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Utils","metadata":{}},{"cell_type":"code","source":"def get_model_params(model_list, n_par=None):\n    # count the number of parameters of a given model\n    if n_par == None:\n        exp_mdl = model_list[0]\n        n_par = 0\n        for name, param in exp_mdl.named_parameters():\n            n_par += len(param.data.reshape(-1))\n\n    # extract the parameters of a given model\n    param_mat = np.zeros((len(model_list), n_par)).astype(\"float32\")\n    for i, mdl in enumerate(model_list):\n        idx = 0\n        for name, param in mdl.named_parameters():\n            temp = param.data.cpu().numpy().reshape(-1)\n            param_mat[i, idx : idx + len(temp)] = temp\n            idx += len(temp)\n    return np.copy(\n        param_mat\n    )  # param_mat =  [[ 0.09114207 -0.10681842  0.10701807 ...  0.07207876  0.00579278   -0.0345436 ]]\n\n\ndef set_model(model, params, device):\n    dict_param = copy.deepcopy(dict(model.named_parameters()))\n    idx = 0\n    for name, param in model.named_parameters():\n        weights = param.data\n        length = len(weights.reshape(-1))\n        dict_param[name].data.copy_(\n            torch.tensor(params[idx : idx + length].reshape(weights.shape)).to(device)\n        )\n        idx += length\n\n    model.load_state_dict(dict_param)\n    return model\n\n\ndef get_acc_loss(\n    data_x, data_y, model, dataset_name, device, w_decay=None, batch_size=50\n):\n    acc_overall = 0\n    loss_overall = 0\n    loss_fn = torch.nn.CrossEntropyLoss(reduction=\"sum\")\n    # batch_size = min(6000, data_x.shape[0])\n    n_tst = data_x.shape[0]\n    tst_gen = data.DataLoader(\n        Dataset(data_x, data_y, dataset_name=dataset_name),\n        batch_size=batch_size,\n        shuffle=False,\n    )\n    model.eval()\n    model = model.to(device)\n    with torch.no_grad():\n        tst_gen_iter = tst_gen.__iter__()\n        for _ in range(int(np.ceil(n_tst / batch_size))):\n            batch_x, batch_y = tst_gen_iter.__next__()\n            batch_x = batch_x.to(device)\n            batch_y = batch_y.to(device)\n            y_pred = model(batch_x)\n\n            loss = loss_fn(y_pred, batch_y.reshape(-1).long())\n            loss_overall += loss.item()\n            # Accuracy calculation\n            y_pred = y_pred.cpu().numpy()\n            y_pred = np.argmax(y_pred, axis=1).reshape(-1)\n            batch_y = batch_y.cpu().numpy().reshape(-1).astype(np.int32)\n            batch_correct = np.sum(y_pred == batch_y)\n            acc_overall += batch_correct\n\n    loss_overall /= n_tst\n    if w_decay != None:\n        # Add L2 loss\n        params = get_model_params([model], n_par=None)\n        loss_overall += w_decay / 2 * np.sum(params * params)\n\n    model.train()\n    return loss_overall, acc_overall / n_tst\n\n\ndef save_performance(\n    communication_rounds,\n    tst_perf_all,\n    algorithm_name,\n    data_obj_name,\n    model_name,\n    n_clients,\n    noiseless,\n    iid_str,\n):\n    plt.figure(figsize=(6, 5))\n    plt.plot(\n        np.arange(communication_rounds) + 1,\n        tst_perf_all[:, 1],\n        label=algorithm_name,\n        linewidth=2.5,\n        color=\"red\",\n    )\n    plt.ylabel(\"Test Accuracy\", fontsize=16)\n    plt.xlabel(\"Communication Rounds\", fontsize=16)\n    plt.legend(fontsize=16, loc=\"lower right\", bbox_to_anchor=(1.015, -0.02))\n    plt.grid()\n    plt.xlim([0, communication_rounds + 1])\n    plt.title(data_obj_name, fontsize=16)\n    plt.xticks(fontsize=16)\n    plt.yticks(fontsize=16)\n    plt.savefig(\n        \"Output/{}/{}_{}cln_{}comm_{}_{}.pdf\".format(\n            data_obj_name,\n            algorithm_name,\n            n_clients,\n            communication_rounds,\n            \"noiseless\" if noiseless else \"noisy\",\n            model_name,\n        ),\n        dpi=1000,\n        bbox_inches=\"tight\",\n    )\n    np.save(\n        \"Output/{}/{}_{}cln_{}comm_{}_{}_{}_tst_perf_all.npy\".format(\n            data_obj_name,\n            algorithm_name,\n            n_clients,\n            communication_rounds,\n            \"noiseless\" if noiseless else \"noisy\",\n            iid_str.lower(),\n            model_name,\n        ),\n        tst_perf_all,\n    )\n\n\ndef evaluate_performance(\n    cent_x,\n    cent_y,\n    tst_x,\n    tst_y,\n    dataset_name,\n    avg_model,\n    all_model,\n    device,\n    tst_perf_sel,\n    trn_perf_sel,\n    tst_perf_all,\n    trn_perf_all,\n    t,\n):\n    loss_tst, acc_tst = get_acc_loss(tst_x, tst_y, avg_model, dataset_name, device)\n    tst_perf_sel[t] = [loss_tst, acc_tst]\n    print(\n        \"\\n**** Communication sel %3d, Test Accuracy: %.4f, Loss: %.4f\"\n        % (t + 1, acc_tst, loss_tst)\n    )\n\n    loss_tst, acc_tst = get_acc_loss(cent_x, cent_y, avg_model, dataset_name, device)\n    trn_perf_sel[t] = [loss_tst, acc_tst]\n    print(\n        \"**** Communication sel %3d, Cent Accuracy: %.4f, Loss: %.4f\"\n        % (t + 1, acc_tst, loss_tst)\n    )\n\n    loss_tst, acc_tst = get_acc_loss(tst_x, tst_y, all_model, dataset_name, device)\n    tst_perf_all[t] = [loss_tst, acc_tst]\n    print(\n        \"**** Communication all %3d, Test Accuracy: %.4f, Loss: %.4f\"\n        % (t + 1, acc_tst, loss_tst)\n    )\n\n    loss_tst, acc_tst = get_acc_loss(cent_x, cent_y, all_model, dataset_name, device)\n    trn_perf_all[t] = [loss_tst, acc_tst]\n    print(\n        \"**** Communication all %3d, Cent Accuracy: %.4f, Loss: %.4f\\n\"\n        % (t + 1, acc_tst, loss_tst)\n    )\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Server","metadata":{}},{"cell_type":"code","source":"def get_model_params(model_list, n_par=None):\n    # count the number of parameters of a given model\n    if n_par == None:\n        exp_mdl = model_list[0]\n        n_par = 0\n        for name, param in exp_mdl.named_parameters():\n            n_par += len(param.data.reshape(-1))\n\n    # extract the parameters of a given model\n    param_mat = np.zeros((len(model_list), n_par)).astype(\"float32\")\n    for i, mdl in enumerate(model_list):\n        idx = 0\n        for name, param in mdl.named_parameters():\n            temp = param.data.cpu().numpy().reshape(-1)\n            param_mat[i, idx : idx + len(temp)] = temp\n            idx += len(temp)\n    return np.copy(\n        param_mat\n    )  # param_mat =  [[ 0.09114207 -0.10681842  0.10701807 ...  0.07207876  0.00579278   -0.0345436 ]]\n\n\ndef set_model(model, params, device):\n    dict_param = copy.deepcopy(dict(model.named_parameters()))\n    idx = 0\n    for name, param in model.named_parameters():\n        weights = param.data\n        length = len(weights.reshape(-1))\n        dict_param[name].data.copy_(\n            torch.tensor(params[idx : idx + length].reshape(weights.shape)).to(device)\n        )\n        idx += length\n\n    model.load_state_dict(dict_param)\n    return model\n\n\ndef get_acc_loss(\n    data_x, data_y, model, dataset_name, device, w_decay=None, batch_size=50\n):\n    acc_overall = 0\n    loss_overall = 0\n    loss_fn = torch.nn.CrossEntropyLoss(reduction=\"sum\")\n    # batch_size = min(6000, data_x.shape[0])\n    n_tst = data_x.shape[0]\n    tst_gen = data.DataLoader(\n        Dataset(data_x, data_y, dataset_name=dataset_name),\n        batch_size=batch_size,\n        shuffle=False,\n    )\n    model.eval()\n    model = model.to(device)\n    with torch.no_grad():\n        tst_gen_iter = tst_gen.__iter__()\n        for _ in range(int(np.ceil(n_tst / batch_size))):\n            batch_x, batch_y = tst_gen_iter.__next__()\n            batch_x = batch_x.to(device)\n            batch_y = batch_y.to(device)\n            y_pred = model(batch_x)\n\n            loss = loss_fn(y_pred, batch_y.reshape(-1).long())\n            loss_overall += loss.item()\n            # Accuracy calculation\n            y_pred = y_pred.cpu().numpy()\n            y_pred = np.argmax(y_pred, axis=1).reshape(-1)\n            batch_y = batch_y.cpu().numpy().reshape(-1).astype(np.int32)\n            batch_correct = np.sum(y_pred == batch_y)\n            acc_overall += batch_correct\n\n    loss_overall /= n_tst\n    if w_decay != None:\n        # Add L2 loss\n        params = get_model_params([model], n_par=None)\n        loss_overall += w_decay / 2 * np.sum(params * params)\n\n    model.train()\n    return loss_overall, acc_overall / n_tst\n\n\ndef save_performance(\n    communication_rounds,\n    tst_perf_all,\n    algorithm_name,\n    data_obj_name,\n    model_name,\n    n_clients,\n    noiseless,\n    iid_str,\n):\n    plt.figure(figsize=(6, 5))\n    plt.plot(\n        np.arange(communication_rounds) + 1,\n        tst_perf_all[:, 1],\n        label=algorithm_name,\n        linewidth=2.5,\n        color=\"red\",\n    )\n    plt.ylabel(\"Test Accuracy\", fontsize=16)\n    plt.xlabel(\"Communication Rounds\", fontsize=16)\n    plt.legend(fontsize=16, loc=\"lower right\", bbox_to_anchor=(1.015, -0.02))\n    plt.grid()\n    plt.xlim([0, communication_rounds + 1])\n    plt.title(data_obj_name, fontsize=16)\n    plt.xticks(fontsize=16)\n    plt.yticks(fontsize=16)\n    plt.savefig(\n        \"Output/{}/{}_{}cln_{}comm_{}_{}.pdf\".format(\n            data_obj_name,\n            algorithm_name,\n            n_clients,\n            communication_rounds,\n            \"noiseless\" if noiseless else \"noisy\",\n            model_name,\n        ),\n        dpi=1000,\n        bbox_inches=\"tight\",\n    )\n    np.save(\n        \"Output/{}/{}_{}cln_{}comm_{}_{}_{}_tst_perf_all.npy\".format(\n            data_obj_name,\n            algorithm_name,\n            n_clients,\n            communication_rounds,\n            \"noiseless\" if noiseless else \"noisy\",\n            iid_str.lower(),\n            model_name,\n        ),\n        tst_perf_all,\n    )\n\n\ndef evaluate_performance(\n    cent_x,\n    cent_y,\n    tst_x,\n    tst_y,\n    dataset_name,\n    avg_model,\n    all_model,\n    device,\n    tst_perf_sel,\n    trn_perf_sel,\n    tst_perf_all,\n    trn_perf_all,\n    t,\n):\n    loss_tst, acc_tst = get_acc_loss(tst_x, tst_y, avg_model, dataset_name, device)\n    tst_perf_sel[t] = [loss_tst, acc_tst]\n    print(\n        \"\\n**** Communication sel %3d, Test Accuracy: %.4f, Loss: %.4f\"\n        % (t + 1, acc_tst, loss_tst)\n    )\n\n    loss_tst, acc_tst = get_acc_loss(cent_x, cent_y, avg_model, dataset_name, device)\n    trn_perf_sel[t] = [loss_tst, acc_tst]\n    print(\n        \"**** Communication sel %3d, Cent Accuracy: %.4f, Loss: %.4f\"\n        % (t + 1, acc_tst, loss_tst)\n    )\n\n    loss_tst, acc_tst = get_acc_loss(tst_x, tst_y, all_model, dataset_name, device)\n    tst_perf_all[t] = [loss_tst, acc_tst]\n    print(\n        \"**** Communication all %3d, Test Accuracy: %.4f, Loss: %.4f\"\n        % (t + 1, acc_tst, loss_tst)\n    )\n\n    loss_tst, acc_tst = get_acc_loss(cent_x, cent_y, all_model, dataset_name, device)\n    trn_perf_all[t] = [loss_tst, acc_tst]\n    print(\n        \"**** Communication all %3d, Cent Accuracy: %.4f, Loss: %.4f\\n\"\n        % (t + 1, acc_tst, loss_tst)\n    )\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Communication ","metadata":{}},{"cell_type":"code","source":"# air communication\nclass AirComp(object):\n    def __init__(self, n_receive_ant, weight_list, transmit_power):\n        self.n_receive_ant = n_receive_ant\n        self.weight_list = weight_list\n        self.transmit_power = transmit_power\n\n        self.need_air_comp = True\n\n    def transmit(self, d, signal, x, f, h, sigma):\n        index = x == 1\n        N = self.n_receive_ant\n        K = self.weight_list[index] # K_m\n        K2 = K**2 # (K_m)^2\n\n        inner = f.conj() @ h[:, index]\n        inner2 = np.abs(inner) ** 2\n\n        g = signal\n\n        # mean and variance\n        mean = np.mean(g, axis = 1)\n        g_bar = K @ mean\n\n        var = np.var(g, axis = 1)\n        var_sqrt = var ** 0.5\n\n        eta = np.min(self.transmit_power * inner2 / K2 / var) # from 17a\n        eta_sqrt = eta ** 0.5\n        b = K * eta_sqrt * var_sqrt * inner.conj() / inner2 # from 17b\n\n        noise_power = sigma * self.transmit_power\n\n        n = (\n            (np.random.randn(N, d) + 1j * np.random.randn(N, d))\n            / (2) ** 0.5\n            * noise_power ** 0.5\n        )\n\n        x_signal = np.tile(b / var_sqrt, (d, 1)).T * (g - np.tile(mean, (d, 1)).T)\n        y = h[:, index] @ x_signal + n\n        w = np.real((f.conj() @ y / eta_sqrt + g_bar)) / np.sum(K) # from 11\n\n        return w if self.need_air_comp else y\n\n# xây dựng kênh truyền\nclass Channel(object):\n    def __init__(\n            self,\n            SNR, # tỉ số tín hiệu trên nhiễu\n            n_clients, # số lượng edge devices\n            location_range, # phạm vi đặt tọa độ của client\n            fc, # tần số sóng mang (free-space path loss)\n            alpha_direct, # hệ số suy hao trên đường truyền (PL trong bài báo)\n            n_RIS_ele, # số lượng phần tử RIS (L trong bài báo)\n            n_receive_ant, # số anten ở PS\n            User_Gain, # antenna gain at user (G_D)\n            x0, # tọa độ ban đầu của thiết bị\n            BS, # vị trí của PS\n            BS_Gain, # antenna gain at PS (G_PS)\n            RIS, # vị trí của RIS\n            RIS_Gain, # antenna gain at RIS (G_RIS)\n            dimen_RIS, # kích thước phần tử RIS\n    ):\n        self.SNR = SNR\n        self.n_clients = n_clients\n        self.location_range = location_range\n        self.fc = fc\n        self.alpha_direct = alpha_direct\n        self.n_RIS_ele = n_RIS_ele\n        self.n_receive_ant = n_receive_ant\n        self.User_Gain = User_Gain\n        self.x = x0\n\n        self.BS = BS\n        self.BS_Gain = BS_Gain\n\n        self.RIS = RIS\n        self.RIS_Gain = RIS_Gain\n        self.dimen_RIS = dimen_RIS\n\n    def generate(self):\n        ref = (1e-10) ** 0.5\n        sigma_n = np.power(10, -self.SNR/10)\n        sigma = sigma_n / ref**2\n\n        # setting 2\n        dx2 = (\n            np.random.rand(int(self.n_clients - np.round(self.n_clients / 2)))\n            * self.location_range + 200\n        ) # phần ở xa\n        dx1 = (\n            np.random.rand(int(np.round(self.n_clients /2 ))) * self.location_range - self.location_range\n        ) # [-location_range, 0]\n\n        dx = np.concatenate((dx1, dx2)) # mảng chứa các tọa độ dx của thiết bị\n        np.random.shuffle(dx)\n\n        dy = np.random.rand(self.n_clients) * 20 - 10\n\n        d_UR = (\n            (dx - self.RIS[0]) ** 2 + (dy - self.RIS[1]) ** 2 + self.RIS[2] ** 2\n        ) ** 0.5 # khoảng cách từ RIS đến User\n\n        d_RB = np.linalg.norm(self.BS - self.RIS) # khoảng cách tử RIS đến PS\n        d_direct = (\n            (dx - self.BS[0]) ** 2 + (dy - self.BS[1]) ** 2 + self.BS[2] ** 2\n        ) ** 0.5 # khoảng cách từ PS đến User\n\n        PL_direct = (\n            self.BS_Gain * self.User_Gain\n            * (3 * 10 ** 8 / self.fc / 4 / np.pi / d_direct) ** self.alpha_direct\n        ) # free-space path loss, device-PS direct channels\n        PL_RIS = (\n            self.BS_Gain * self.User_Gain * self.RIS_Gain\n            * self.n_RIS_ele ** 2\n            * self.dimen_RIS ** 2\n            / 4\n            / np.pi\n            * (3 * 10**8 / self.fc / 4 / np.pi / d_UR) ** 2\n            * (3 * 10**8 / self.fc / 4 / np.pi / d_RB) ** 2\n        )\n\n        h_d = (\n            np.random.randn(self.n_receive_ant, self.n_clients)\n            + 1j * np.random.randn(self.n_receive_ant, self.n_clients)\n        ) / 2 ** 0.5 # small-scale fading coefficients\n        h_d = h_d @ np.diag(PL_direct**0.5) / ref # channel coefficients\n\n        H_RB = (\n            np.random.randn(self.n_receive_ant, self.n_RIS_ele)\n            + 1j * np.random.randn(self.n_receive_ant, self.n_RIS_ele)\n        ) / 2**0.5 # RIS to PS\n\n        h_UR = (\n            np.random.randn(self.n_RIS_ele, self.n_clients)\n            + 1j * np.random.randn(self.n_RIS_ele, self.n_clients)\n        ) / 2**0.5\n        h_UR = h_UR @ np.diag(PL_RIS**0.5) / ref # User to PS\n\n        G = np.zeros(\n            [self.n_receive_ant, self.n_RIS_ele, self.n_clients], dtype = complex\n        ) # các kênh truyền user -> 1 phần tử trên RIS -> 1 anten của PS\n        for j in range(self.n_clients): # với mỗi user\n            G[:, :, j] = H_RB @ np.diag(h_UR[:, j]) # RIS to PS multiply User(j) to RIS\n\n        return h_d, G, self.x, sigma # kênh trực tiếp, kênh gián tiếp, tọa độ user, noise\n\n# optimize\nclass Gibbs(object):\n    def __init__(\n            self,\n            n_clients, # số lượng user\n            n_receive_ant, # số anten tại PS\n            n_RIS_ele, # số phần tử của RIS\n            Jmax, # số vòng Gibbs sampling\n            weight_list, # trọng số của thiết bị (K_m)\n            tau,\n            nit,\n            threshold,\n    ):\n        self.n_clients = n_clients\n        self.n_receive_ant = n_receive_ant\n        self.n_RIS_ele = n_RIS_ele\n        self.Jmax = Jmax\n        self.weight_list = weight_list\n\n        # SCA_based optimization\n        self.tau = tau\n        self.nit = nit\n        self.threshold = threshold\n\n    def optimize(self, h_d, G, x0, sigma):\n        x_store, obj_new, f_store, theta_store = self.sampling(h_d, G, x0, sigma) # tối ưu luân phiên từng biến\n\n        x_optim = x_store[self.Jmax]\n        f_optim = f_store[:, self.Jmax]\n        theta_optim = theta_store[:, self.Jmax]\n        h_optim = np.zeros([self.n_receive_ant, self.n_clients], dtype = complex)\n\n        for i in range(self.n_clients):\n            h_optim[:, i] = h_d[:, i] + G[:, :, i] @ theta_optim\n\n        return x_optim, f_optim, h_optim\n\n    # algorithm 2\n    def sampling(self, h_d, G, x0, sigma):\n        N = self.n_receive_ant\n        L = self.n_RIS_ele\n        M = self.n_clients\n\n        K = self.weight_list/ np.mean(self.weight_list) # chuẩn hóa K\n        K2 = K ** 2\n        Ksum2 = sum(K)**2 # K^2\n        x = x0\n\n        #\n        obj_new = np.zeros(self.Jmax + 1)\n        f_store = np.zeros([N, self.Jmax + 1], dtype = complex)\n        theta_store = np.zeros([L, self.Jmax + 1], dtype = complex)\n        x_store = np.zeros([self.Jmax + 1, M], dtype = int)\n\n        # first loop\n        ind = 0\n        [obj_new[ind], x_store[ind, :], f, theta] = self.find_obj_inner(\n            x, K, K2, Ksum2, h_d, G, None, None, sigma\n        )\n\n        theta_store[:, ind] = copy.deepcopy(theta)\n        f_store[:, ind] = copy.deepcopy(f)\n        beta = min(1, obj_new[ind])\n        alpha = 0.9\n\n        f_loop = np.tile(f, (M+1, 1))\n        theta_loop = np.tile(theta, (M+1, 1))\n\n        for j in range(self.Jmax):\n\n            # store possible transition solution and their obj\n            X_sample = np.zeros([M+1, M], dtype = int)\n            Temp = np.zeros(M+1)\n\n            # first transition -> no change\n            X_sample[0, :] = copy.deepcopy(x)\n            Temp[0] = copy.deepcopy(obj_new[ind])\n            f_loop[0] = copy.deepcopy(f)\n            theta_loop[0] = copy.deepcopy(theta)\n\n            # 2 - M+1 transition, change only 1 position\n            for m in range(M):\n\n                # flip the m-th position\n                x_sam = copy.deepcopy(x)\n                x_sam[m] = copy.deepcopy((x_sam[m] + 1) % 2)\n                X_sample[m+1, :] = copy.deepcopy(x_sam)\n                Temp[m+1], _, f_loop[m+1], theta_loop[m+1] = self.find_obj_inner(\n                    x_sam,\n                    K,\n                    K2,\n                    Ksum2,\n                    h_d,\n                    G,\n                    f_loop[m+1],\n                    theta_loop[m+1],\n                    sigma,\n                )\n            temp2 = Temp\n\n            Lambda = np.exp(-1 * temp2/beta)\n            Lambda = Lambda / sum(Lambda)\n            while np.isnan(Lambda).any():\n                beta = beta / alpha\n                Lambda = np.exp(-1.0 * temp2 / beta)\n                Lambda = Lambda/sum(Lambda)\n\n            kk_prime = np.random.choice(M+1, p = Lambda)\n            x = copy.deepcopy(X_sample[kk_prime, :])\n            f = copy.deepcopy(f_loop[kk_prime])\n            theta = copy.deepcopy(theta_loop[kk_prime])\n            ind += 1\n            obj_new[ind] = copy.deepcopy(Temp[kk_prime])\n            x_store[ind, :] = copy.deepcopy(x)\n            theta_store[:, ind] = copy.deepcopy(theta)\n            f_store[:, ind] = copy.deepcopy(f)\n\n            beta = max(alpha * beta, 1e-4)\n\n        return x_store, obj_new, f_store, theta_store\n\n    # đánh giá thiết bị được chọn\n    def find_obj_inner(self, x, K, K2, Ksum2, h_d, G, f0, theta0, sigma):\n        N = self.n_receive_ant\n        L = self.n_RIS_ele\n        M = self.n_clients\n\n        if sum(x) == 0:\n            obj = np.inf\n\n            theta = np.ones([L], dtype=complex)\n            f = h_d[:, 0] / np.linalg.norm(h_d[:, 0])\n        else:\n            index = x == 1\n\n            f, theta, _ = self.sca_fmincon(\n                h_d[:, index], G[:, :, index], f0, theta0, x, K2[index]\n            )\n\n            h = np.zeros([N, M], dtype=complex)\n            for i in range(M):\n                h[:, i] = h_d[:, i] + G[:, :, i] @ theta\n            gain = K2 / (np.abs(np.conjugate(f) @ h) ** 2) * sigma\n            obj = (\n                np.max(gain[index]) / (sum(K[index])) ** 2\n                + 4 / Ksum2 * (sum(K[~index])) ** 2\n            )\n        return obj, x, f, theta\n\n    # Algorithm 1\n    def sca_fmincon(self, h_d, G, f, theta, x, K2):  # (25)\n        N = self.n_receive_ant\n        L = self.n_RIS_ele\n        I = sum(x)\n\n        if theta is None:\n            theta = np.ones([L], dtype=complex)\n        result = np.zeros(self.nit)\n        h = np.zeros([N, I], dtype=complex)\n        for i in range(I):\n            h[:, i] = h_d[:, i] + G[:, :, i] @ theta\n\n        if f is None:\n            f = h[:, 0] / np.linalg.norm(h[:, 0])\n\n        obj = min(np.abs(np.conjugate(f) @ h) ** 2 / K2)\n\n        for it in range(self.nit):\n            obj_pre = copy.deepcopy(obj)\n            a = np.zeros([N, I], dtype=complex)\n            b = np.zeros([L, I], dtype=complex)\n            c = np.zeros([1, I], dtype=complex)\n            F_cro = np.outer(f, np.conjugate(f))\n            for i in range(I):  # (26)\n                a[:, i] = (\n                    self.tau * K2[i] * f + np.outer(h[:, i], np.conjugate(h[:, i])) @ f\n                )\n\n                b[:, i] = (\n                    self.tau * K2[i] * theta + G[:, :, i].conj().T @ F_cro @ h[:, i]\n                )\n\n                c[:, i] = (\n                    np.abs(np.conjugate(f) @ h[:, i]) ** 2\n                    + 2 * self.tau * K2[i] * (L + 1)\n                    + 2\n                    * np.real(\n                        (theta.conj().T) @ (G[:, :, i].conj().T) @ F_cro @ h[:, i]\n                    )\n                )\n\n            # convex optimization\n            mu = cp.Variable(I, nonneg=True)\n            obj = cp.Minimize(\n                cp.real(2 * cp.norm(a @ mu) + 2 * cp.norm(b @ mu, 1) - c @ mu)\n            )\n            prob = cp.Problem(obj, [K2 @ mu == 1])\n            mu.value = 1 / K2\n            prob.solve(solver='ECOS')\n\n            fn = a @ mu.value\n            thetan = b @ mu.value\n            fn = fn / np.linalg.norm(fn)\n            f = fn\n\n            thetan = thetan / np.abs(thetan)\n            theta = thetan\n\n            for i in range(I):\n                h[:, i] = h_d[:, i] + G[:, :, i] @ theta\n            obj = min(np.abs(np.conjugate(f) @ h) ** 2 / K2)  # (24)\n            result[it] = copy.deepcopy(obj)\n\n            if np.abs(obj - obj_pre) / min(1, abs(obj)) <= self.threshold:\n                break\n\n        result = result[0:it]\n        return f, theta, result","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Client","metadata":{}},{"cell_type":"code","source":"class Client:\n    def __init__(\n        self,\n        algorithm,  # local training algorithm\n        device,\n        weight,\n        train_data_X,\n        train_data_Y,\n        model,\n        client_param,\n        malicious=False,          # NEW: whether this client is adversary\n        attack_params=None,       # NEW: dict with keys {'poison_fraction','epsilon','poison_labels'}\n        client_id=None,\n    ):\n        self.algorithm = algorithm\n        self.device = device\n        self.weight = weight\n        self.train_data_X = train_data_X\n        self.train_data_Y = train_data_Y\n        self.model = model\n        self.client_param = client_param\n        self.malicious = malicious\n        self.attack_params = attack_params or {}\n        self.client_id = client_id\n\n    def local_train(self, inputs: dict):\n        # Pass inputs through; algorithm.local_train will inspect inputs.get('attack_params')\n        self.algorithm.local_train(self, inputs)\n\n    def aggregate(self, algorithm):\n        self.algorithm = algorithm","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Adversary FGSM","metadata":{}},{"cell_type":"code","source":"def fgsm_image_attack(model, x_batch, y_batch, epsilon=2/255.0, device='cpu', targeted=False, target_labels=None):\n    \"\"\"\n    Single-step FGSM on image batch.\n    - model: torch.nn.Module (in eval/training mode ok)\n    - x_batch: torch.Tensor shape (B, C, H, W), values in [0,1] or normalized as your transforms\n    - y_batch: torch.Tensor shape (B,) (long)\n    - epsilon: float (perturbation magnitude)\n    - targeted: if True, uses target_labels (tensor) to reduce loss on target (less common)\n    Returns: torch.Tensor (adversarial images) same device as x_batch\n    \"\"\"\n    model.eval()  # we compute gradients w.r.t. inputs only\n    x = x_batch.clone().detach().to(device)\n    x.requires_grad = True\n    y = y_batch.clone().detach().to(device)\n    logits = model(x)\n    if targeted and target_labels is not None:\n        # targeted: minimize loss for target -> use negative grad\n        loss = F.cross_entropy(logits, target_labels)\n        factor = -1.0\n    else:\n        loss = F.cross_entropy(logits, y)\n        factor = 1.0\n    model.zero_grad()\n    loss.backward()\n    grad = x.grad.data\n    x_adv = x + factor * epsilon * torch.sign(grad)\n    # clamp according to underlying preprocessing.\n    # If your data is in [0,1] (ToTensor) then clamp 0..1; if normalized, adjust accordingly.\n    x_adv = torch.clamp(x_adv, 0.0, 1.0)\n    x_adv = x_adv.detach()\n    return x_adv","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Algorithm","metadata":{}},{"cell_type":"code","source":"# algorithm_base :\nclass Algorithm:\n    # constructor\n    def __init__(\n        self,\n        name, # tên thuật toán\n        lr, # learning rate\n        lr_decay_per_round, # tỷ lệ giảm learning rate sau mỗi round\n        batch_size,\n        epoch, # số epoch training local\n        weight_decay, # hệ số L2 penalty - chống overfitting\n        model_func, # hàm khởi tạo model\n        n_param, # số lượng tham số\n        max_norm, # max gradient (nếu ||g||_2 > max_norm, g = g * (max_norm/||g||_2) )\n        noiseless, # cờ bật/tắt nhiễu\n        dataset_name,\n        save_period, # chu kì lưu model\n        print_per, # sau mỗi n round sẽ in\n  ):\n        self.name = name\n        self.lr = lr\n        self.lr_decay_per_round = lr_decay_per_round\n        self.batch_size = batch_size\n        self.epoch = epoch\n        self.weight_decay = weight_decay\n        self.model_func = model_func\n        self.n_param = n_param\n        self.max_norm = max_norm\n        self.noiseless = noiseless\n        self.dataset_name = dataset_name\n        self.save_period = save_period\n        self.print_per = print_per\n\n    @abstractmethod\n    def local_train(self):\n        raise NotImplementedError(f\"Subclass {self.__class__.__name__} must implement this method for client local training.\")\n\n    abstractmethod\n    def aggregate(self):\n        raise NotImplementedError(f\"Subclass {self.__class__.__name__} must implement this method for server aggregation. \")\n\nclass AlgorithmFactory:\n    def __init__(self, args):\n        self.args = args\n\n    def create_algorithm(self, algorithm_name) -> Algorithm:\n        algorithm = None\n\n        if algorithm_name == \"FedDyn\":\n            alpha_coef = 1e-2\n\n            algorithm = FedDyn(\n                self.args.lr,\n                self.args.lr_decay_per_round,\n                self.args.batch_size,\n                self.args.epoch,\n                self.args.weight_decay,\n                self.args.model_func,\n                self.args.n_param,\n                self.args.max_norm,\n                self.args.noiseless,\n                self.args.data_obj.dataset,\n                self.args.save_period,\n                self.args.print_per,\n                alpha_coef,\n            )\n\n        elif algorithm_name == \"FedProx\":\n            mu = 1e-4\n\n            algorithm = FedProx(\n                self.args.lr,\n                self.args.lr_decay_per_round,\n                self.args.batch_size,\n                self.args.epoch,\n                self.args.weight_decay,\n                self.args.model_func,\n                self.args.n_param,\n                self.args.max_norm,\n                self.args.noiseless,\n                self.args.data_obj.dataset,\n                self.args.save_period,\n                self.args.print_per,\n                mu,\n            )\n\n        elif algorithm_name == \"SCAFFOLD\":\n            n_data_per_client = (\n                np.concatenate(self.args.data_obj.clnt_x, axis=0).shape[0]\n                / self.args.n_clients\n            )\n            n_iter_per_epoch = np.ceil(n_data_per_client / self.args.batch_size)\n            n_minibatch = (self.args.epoch * n_iter_per_epoch).astype(np.int64)\n            self.args.print_per = self.args.print_per * n_iter_per_epoch\n            global_learning_rate = 1\n\n            algorithm = SCAFFOLD(\n                self.args.lr,\n                self.args.lr_decay_per_round,\n                self.args.batch_size,\n                self.args.epoch,\n                self.args.weight_decay,\n                self.args.model_func,\n                self.args.n_param,\n                self.args.max_norm,\n                self.args.noiseless,\n                self.args.data_obj.dataset,\n                self.args.save_period,\n                self.args.print_per,\n                n_minibatch,\n                global_learning_rate,\n            )\n\n        elif algorithm_name == \"FedAvg\":\n            algorithm = FedAvg(\n                self.args.lr,\n                self.args.lr_decay_per_round,\n                self.args.batch_size,\n                self.args.epoch,\n                self.args.weight_decay,\n                self.args.model_func,\n                self.args.n_param,\n                self.args.max_norm,\n                self.args.noiseless,\n                self.args.data_obj.dataset,\n                self.args.save_period,\n                self.args.print_per,\n            )\n\n        else:\n            raise ValueError(f\"Unknown algorithm name: {self.algorithm_name}\")\n\n        return algorithm\n\n# FedAvg\nclass FedAvg(Algorithm):\n    def __init__(\n        self,\n        lr,\n        lr_decay_per_round,\n        batch_size,\n        epoch,\n        weight_decay,\n        model_func,\n        n_param,\n        max_norm,\n        noiseless,\n        dataset_name,\n        save_period,\n        print_per,\n    ):\n        super().__init__(\n            \"FedAvg\",\n            lr,\n            lr_decay_per_round,\n            batch_size,\n            epoch,\n            weight_decay,\n            model_func,\n            n_param,\n            max_norm,\n            noiseless,\n            dataset_name,\n            save_period,\n            print_per,\n        )\n\n    def local_train(self, client: Client, inputs: dict):\n        self.device = client.device\n\n        client.model = self.model_func().to(self.device)\n        client.model.load_state_dict(\n            copy.deepcopy(dict(inputs[\"avg_model\"].named_parameters()))\n        )\n\n        for params in client.model.parameters():\n            parems.requires_grad = True\n\n        print(\"client model parameters: \", get_model_params([client.model], self.n_param)[0],)\n\n        client.model = self.__train_model(\n            client.model,\n            client.train_data_X,\n            client.train_data_Y,\n            inputs[\"curr_round\"]\n        )\n        updated_param = get_model_params([client.model], self.n_param)[0]\n        print(\"updated model parameters: \", updated_param)\n\n        client.client_param = updated_param\n\n    def __train_model(self, model, trn_x, trn_y, curr_round):\n        decayed_lr = self.lr * (self.lr_decay_per_round ** curr_round)\n\n        n_trn = trn_x.shape[0]\n        trn_gen = data.DataLoader(\n            Dataset(trn_x, trn_y, train = True, dataset_name = self.dataset_name),\n            batch_size = self.batch_size,\n            shuffle = True,\n        )\n        loss_fn = torch.nn.CrossEntropyLoss(reduction = \"sum\")\n\n        optimizer = torch.optim.SGD(\n            model.parameters(), lr = decayed_lr, weight_decay = self.weight_decay\n        )\n        model.train()\n        model = model.to(self.device)\n\n        for e in range(self.epoch):\n            # training\n            trn_gen_iter = trn_gen.__iter__()\n            for _ in range (int(np.ceil(n_trn/self.batch_size))):\n                batch_x, batch_y = trn_gen_iter.__next__()\n                batch_x = batch_x.to(self.device)\n                batch_y = batch_y.to(self.device)\n\n                y_pred = model(batch_x)\n                loss = loss_fn(y_pred, batch_y.reshape(-1).long())\n                loss = loss / list(batch_y.size())[0]\n\n                optimizer.zero_grad()\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(\n                    parameters = model.parameters(), max_norm = self.max_norm\n                )\n                optimizer.step()\n\n            if (e + 1) % self.print_per == 0:\n                loss_trn, acc_trn = get_acc_loss(\n                    trn_x, trn_y, model, self.dataset_name, self.weight_decay\n                )\n                print(\n                    \"epoch %3d, training accuracy: %.4f, loss: %.4f\" % (e+1, acc_trn, loss_trn)\n                )\n                model.train()\n\n        for params in model.parameters():\n            params.requires_grad = False\n        model.eval()\n\n        return model\n\n    def aggreate(self, server: Server, inputs: dict):\n        clients_list = inputs[\"clients_list\"]\n        selected_clients_idx = inputs[\"selected_clients_idx\"]\n        weight_list = inputs[\"weight_list\"]\n\n        clients_param_list = np.array([client.client_param for client in clinets_list])\n        weight_list = weight_list.reshape((-1, 1))\n        print(\"weight_list.shape = \", weight_list.shape)\n\n        avg_model_param = (\n            inputs[\"avg_model_param\"]\n            if not self.noiseless\n            else np.sum(\n                clients_param_list[selected_clients_idx]\n                * weight_list[selected_clients_idx]\n                / np.sum(weight_list[selected_clients_idx]),\n                axis = 0,\n            )\n        )\n\n        print(\"avg_model_param = \", avg_model_param)\n\n        server.avg_model = set_model(self.model_func(), avg_model_param, server.device)\n        server.all_model = set_model(\n            self.model_func(),\n            np.sum(clients_param_list * weight_list / np.sum(weight_list), axis = 0),\n            server.device,\n        )\n\n# FedDyn\nclass FedDyn(Algorithm):\n    def __init__(\n        self,\n        lr,\n        lr_decay_per_round,\n        batch_size,\n        epoch,\n        weight_decay,\n        model_func,\n        n_param,\n        max_norm,\n        noiseless,\n        dataset_name,\n        save_period,\n        print_per,\n        alpha_coef,\n    ):\n        super().__init__(\n            \"FedDyn\",\n            lr,\n            lr_decay_per_round,\n            batch_size,\n            epoch,\n            weight_decay,\n            model_func,\n            n_param,\n            max_norm,\n            noiseless,\n            dataset_name,\n            save_period,\n            print_per,\n        )\n\n        self.alpha_coef = alpha_coef\n\n    # override\n    def local_train(self, client: Client, inputs: dict):\n        self.device = client.device\n\n        client.model = self.model_func().to(self.device)\n        model = client.model\n        # Warm start from current avg model\n        model.load_state_dict(\n            copy.deepcopy(dict(inputs[\"cloud_model\"].named_parameters()))\n        )\n        for params in model.parameters():\n            params.requires_grad = True\n\n        # Scale down\n        alpha_coef_adpt = self.alpha_coef / client.weight  # adaptive alpha coef\n        local_param_list_curr = torch.tensor(\n            inputs[\"local_param\"], dtype=torch.float32, device=self.device\n        )  # = local_grad_vector\n        print(\"local_param_list_curr = \", local_param_list_curr)\n        print(\"cloud_model_param_tensor = \", inputs[\"cloud_model_param_tensor\"])\n        client.model = self.__train_model(\n            model,\n            alpha_coef_adpt,\n            inputs[\"cloud_model_param_tensor\"],\n            local_param_list_curr,\n            client.train_data_X,\n            client.train_data_Y,\n            inputs[\"curr_round\"],\n        )\n        curr_model_par = get_model_params([client.model], self.n_param)[\n            0\n        ]  # get the model parameter after running FedDyn\n        print(\"curr_model_par = \", curr_model_par)\n\n        # No need to scale up hist terms. They are -\\nabla/alpha and alpha is already scaled.\n        inputs[\"local_param\"] += (\n            curr_model_par - inputs[\"cloud_model_param\"]\n        )  # after training, dynamically update the weight with the cloud model parameters\n\n        client.client_param = curr_model_par\n\n    def __train_model(\n        self,\n        model,\n        alpha_coef_adpt,\n        avg_mdl_param,\n        local_grad_vector,\n        trn_x,\n        trn_y,\n        curr_round,\n    ):\n        decayed_lr = self.lr * (self.lr_decay_per_round**curr_round)\n\n        n_trn = trn_x.shape[0]\n        trn_gen = data.DataLoader(\n            Dataset(trn_x, trn_y, train=True, dataset_name=self.dataset_name),\n            batch_size=self.batch_size,\n            shuffle=True,\n        )\n        loss_fn = torch.nn.CrossEntropyLoss(reduction=\"sum\")\n\n        optimizer = torch.optim.SGD(\n            model.parameters(),\n            lr=decayed_lr,\n            weight_decay=alpha_coef_adpt + self.weight_decay,\n        )\n        model.train()\n        model = model.to(self.device)\n\n        for e in range(self.epoch):\n            # Training\n            epoch_loss = 0\n            trn_gen_iter = trn_gen.__iter__()\n            for _ in range(int(np.ceil(n_trn / self.batch_size))):\n                batch_x, batch_y = trn_gen_iter.__next__()\n                batch_x = batch_x.to(self.device)\n                batch_y = batch_y.to(self.device)\n\n                y_pred = model(batch_x)\n\n                ## Get f_i estimate\n                loss_f_i = loss_fn(y_pred, batch_y.reshape(-1).long())\n                loss_f_i = loss_f_i / list(batch_y.size())[0]\n\n                # Get linear penalty on the current parameter estimates\n                local_par_list = None\n                for param in model.parameters():\n                    if not isinstance(local_par_list, torch.Tensor):\n                        # Initially nothing to concatenate\n                        local_par_list = param.reshape(-1)\n                    else:\n                        local_par_list = torch.cat(\n                            (local_par_list, param.reshape(-1)), 0\n                        )\n\n                loss_algo = alpha_coef_adpt * torch.sum(\n                    local_par_list * (-avg_mdl_param + local_grad_vector)\n                )\n                loss = loss_f_i + loss_algo\n\n                optimizer.zero_grad()\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(\n                    parameters=model.parameters(), max_norm=self.max_norm\n                )  # Clip gradients\n                optimizer.step()\n                epoch_loss += loss.item() * list(batch_y.size())[0]\n\n            if (e + 1) % self.print_per == 0:\n                epoch_loss /= n_trn\n                if self.weight_decay != None:\n                    # Add L2 loss to complete f_i\n                    params = get_model_params([model], self.n_param)\n                    epoch_loss += (\n                        (alpha_coef_adpt + self.weight_decay)\n                        / 2\n                        * np.sum(params * params)\n                    )\n                print(\"Epoch %3d, Training Loss: %.4f\" % (e + 1, epoch_loss))\n                model.train()\n\n        # Freeze model\n        for params in model.parameters():\n            params.requires_grad = False\n        model.eval()\n\n        return model\n\n    # override\n    def aggregate(self, server: Server, inputs: dict):\n        clients_list = inputs[\"clients_list\"]\n        selected_clnts_idx = inputs[\"selected_clnts_idx\"]\n\n        clients_param_list = np.array([client.client_param for client in clients_list])\n\n        avg_mdl_param = (\n            inputs[\"avg_mdl_param\"]\n            if not self.noiseless\n            else np.mean(clients_param_list[selected_clnts_idx], axis=0)\n        )\n\n        print(\"avg_mdl_param = \", avg_mdl_param)\n        # print(\"n_param = \", self.n_param)\n        # print(\"avg_mdl_param.shape = \", avg_mdl_param.shape)\n\n        inputs[\"cloud_model_param\"] = avg_mdl_param + np.mean(\n            inputs[\"local_param_list\"], axis=0\n        )\n\n        server.avg_model = set_model(self.model_func(), avg_mdl_param, server.device)\n        server.all_model = set_model(\n            self.model_func(), np.mean(clients_param_list, axis=0), server.device\n        )\n        inputs[\"cloud_model\"] = set_model(\n            self.model_func().to(server.device),\n            inputs[\"cloud_model_param\"],\n            server.device,\n        )\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"import os\n\nimport numpy as np\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\n\n\nclass DatasetObject:\n    def __init__(self, dataset, n_client, rule, unbalanced_sgm=0, rule_arg=\"\"):\n        self.dataset = dataset\n        self.n_client = n_client\n        self.rule = rule\n        self.rule_arg = rule_arg\n        rule_arg_str = rule_arg if isinstance(rule_arg, str) else \"%.3f\" % rule_arg\n        self.name = \"%s_%d_%s_%s\" % (\n            self.dataset,\n            self.n_client,\n            self.rule,\n            rule_arg_str,\n        )\n        self.name += \"_%f\" % unbalanced_sgm if unbalanced_sgm != 0 else \"\"\n        self.unbalanced_sgm = unbalanced_sgm\n        self.data_path = \"Data\"\n        self.set_data()\n\n    def set_data(self):\n        # Prepare data if not ready\n        if not os.path.exists(\"%s/%s\" % (self.data_path, self.name)):\n            # Get Raw data\n            if self.dataset == \"mnist\":\n                transform = transforms.Compose(\n                    [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n                )\n                trnset = torchvision.datasets.MNIST(\n                    root=\"%s/Raw\" % self.data_path,\n                    train=True,\n                    download=True,\n                    transform=transform,\n                )\n                tstset = torchvision.datasets.MNIST(\n                    root=\"%s/Raw\" % self.data_path,\n                    train=False,\n                    download=True,\n                    transform=transform,\n                )\n\n                trn_load = torch.utils.data.DataLoader(\n                    trnset, batch_size=len(trnset), shuffle=False, num_workers=1\n                )\n                tst_load = torch.utils.data.DataLoader(\n                    tstset, batch_size=len(tstset), shuffle=False, num_workers=1\n                )\n                self.channels = 1\n                self.width = 28\n                self.height = 28\n                self.n_cls = 10\n\n            elif self.dataset == \"cifar10\":\n                transform = transforms.Compose(\n                    [\n                        transforms.ToTensor(),\n                        transforms.Normalize(\n                            mean=[0.491, 0.482, 0.447], std=[0.247, 0.243, 0.262]\n                        ),\n                    ]\n                )\n\n                trnset = torchvision.datasets.CIFAR10(\n                    root=\"%s/Raw\" % self.data_path,\n                    train=True,\n                    download=True,\n                    transform=transform,\n                )\n                tstset = torchvision.datasets.CIFAR10(\n                    root=\"%s/Raw\" % self.data_path,\n                    train=False,\n                    download=True,\n                    transform=transform,\n                )\n\n                trn_load = torch.utils.data.DataLoader(\n                    trnset, batch_size=len(trnset), shuffle=False, num_workers=1\n                )\n                tst_load = torch.utils.data.DataLoader(\n                    tstset, batch_size=len(tstset), shuffle=False, num_workers=1\n                )\n                self.channels = 3\n                self.width = 32\n                self.height = 32\n                self.n_cls = 10\n\n            elif self.dataset == \"cifar100\":\n                print(self.dataset)\n                # mean and std are validated here: https://gist.github.com/weiaicunzai/e623931921efefd4c331622c344d8151\n                transform = transforms.Compose(\n                    [\n                        transforms.ToTensor(),\n                        transforms.Normalize(\n                            mean=[0.5071, 0.4867, 0.4408], std=[0.2675, 0.2565, 0.2761]\n                        ),\n                    ]\n                )\n                trnset = torchvision.datasets.CIFAR100(\n                    root=\"%s/Raw\" % self.data_path,\n                    train=True,\n                    download=True,\n                    transform=transform,\n                )\n                tstset = torchvision.datasets.CIFAR100(\n                    root=\"%s/Raw\" % self.data_path,\n                    train=False,\n                    download=True,\n                    transform=transform,\n                )\n                trn_load = torch.utils.data.DataLoader(\n                    trnset, batch_size=len(trnset), shuffle=False, num_workers=0\n                )\n                tst_load = torch.utils.data.DataLoader(\n                    tstset, batch_size=len(tstset), shuffle=False, num_workers=0\n                )\n                self.channels = 3\n                self.width = 32\n                self.height = 32\n                self.n_cls = 100\n\n            elif self.dataset == \"emnist\":\n                transform = transforms.Compose(\n                    [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n                )\n                trnset = torchvision.datasets.EMNIST(\n                    root=\"%s/Raw\" % self.data_path,\n                    split=\"letters\",\n                    train=True,\n                    download=True,\n                    transform=transform,\n                )\n                tstset = torchvision.datasets.EMNIST(\n                    root=\"%s/Raw\" % self.data_path,\n                    split=\"letters\",\n                    train=False,\n                    download=True,\n                    transform=transform,\n                )\n\n                # filter the labels with limitation of 10\n                filtered_indices = trnset.targets.clone().detach() <= 10\n                trnset.targets = trnset.targets[filtered_indices] - 1\n                trnset.data = trnset.data[filtered_indices]\n\n                filtered_indices = tstset.targets.clone().detach() <= 10\n                tstset.targets = tstset.targets[filtered_indices] - 1\n                tstset.data = tstset.data[filtered_indices]\n\n                trn_load = torch.utils.data.DataLoader(\n                    trnset, batch_size=len(trnset), shuffle=False, num_workers=1\n                )\n                tst_load = torch.utils.data.DataLoader(\n                    tstset, batch_size=len(tstset), shuffle=False, num_workers=1\n                )\n                self.channels = 1\n                self.width = 28\n                self.height = 28\n                self.n_cls = 10\n\n            trn_itr = trn_load.__iter__()\n            tst_itr = tst_load.__iter__()\n            # labels are of shape (n_data,)\n            trn_x, trn_y = trn_itr.__next__()\n            tst_x, tst_y = tst_itr.__next__()\n\n            trn_x = trn_x.numpy()\n            trn_y = trn_y.numpy().reshape(-1, 1)\n            tst_x = tst_x.numpy()\n            tst_y = tst_y.numpy().reshape(-1, 1)\n\n            # Shuffle Data\n            rand_perm = np.random.permutation(len(trn_y))\n            trn_x = trn_x[rand_perm]\n            trn_y = trn_y[rand_perm]\n\n            self.trn_x = trn_x\n            self.trn_y = trn_y\n            self.tst_x = tst_x\n            self.tst_y = tst_y\n\n            ###\n            n_data_per_clnt = int((len(trn_y)) / self.n_client)\n            if self.unbalanced_sgm != 0:\n                # Draw from lognormal distribution\n                clnt_data_list = np.random.lognormal(\n                    mean=np.log(n_data_per_clnt),\n                    sigma=self.unbalanced_sgm,\n                    size=self.n_client,\n                )\n                clnt_data_list = (\n                    clnt_data_list / np.sum(clnt_data_list) * len(trn_y)\n                ).astype(int)\n                diff = np.sum(clnt_data_list) - len(trn_y)\n\n                # Add/Subtract the excess number starting from first client\n                if diff != 0:\n                    for clnt_i in range(self.n_client):\n                        if clnt_data_list[clnt_i] > diff:\n                            clnt_data_list[clnt_i] -= diff\n                            break\n            else:\n                clnt_data_list = (np.ones(self.n_client) * n_data_per_clnt).astype(int)\n            ###\n\n            if self.rule == \"dirichlet\":\n                cls_priors = np.random.dirichlet(\n                    alpha=[self.rule_arg] * self.n_cls, size=self.n_client\n                )\n                prior_cumsum = np.cumsum(cls_priors, axis=1)\n                idx_list = [np.where(trn_y == i)[0] for i in range(self.n_cls)]\n                cls_amount = [len(idx_list[i]) for i in range(self.n_cls)]\n\n                clnt_x = [\n                    np.zeros(\n                        (clnt_data_list[clnt__], self.channels, self.height, self.width)\n                    ).astype(np.float32)\n                    for clnt__ in range(self.n_client)\n                ]\n                clnt_y = [\n                    np.zeros((clnt_data_list[clnt__], 1)).astype(np.int64)\n                    for clnt__ in range(self.n_client)\n                ]\n\n                while np.sum(clnt_data_list) != 0:\n                    curr_clnt = np.random.randint(self.n_client)\n                    # If current node is full resample a client\n                    print(\"Remaining Data: %d\" % np.sum(clnt_data_list))\n                    if clnt_data_list[curr_clnt] <= 0:\n                        continue\n                    clnt_data_list[curr_clnt] -= 1\n                    curr_prior = prior_cumsum[curr_clnt]\n                    while True:\n                        cls_label = np.argmax(np.random.uniform() <= curr_prior)\n                        # Redraw class label if trn_y is out of that class\n                        if cls_amount[cls_label] <= 0:\n                            continue\n                        cls_amount[cls_label] -= 1\n                        clnt_x[curr_clnt][clnt_data_list[curr_clnt]] = trn_x[\n                            idx_list[cls_label][cls_amount[cls_label]]\n                        ]\n                        clnt_y[curr_clnt][clnt_data_list[curr_clnt]] = trn_y[\n                            idx_list[cls_label][cls_amount[cls_label]]\n                        ]\n\n                        break\n\n                clnt_x = np.asarray(clnt_x)\n                clnt_y = np.asarray(clnt_y)\n\n                cls_means = np.zeros((self.n_client, self.n_cls))\n                for clnt in range(self.n_client):\n                    for cls in range(self.n_cls):\n                        cls_means[clnt, cls] = np.mean(clnt_y[clnt] == cls)\n                prior_real_diff = np.abs(cls_means - cls_priors)\n                print(\"--- Max deviation from prior: %.4f\" % np.max(prior_real_diff))\n                print(\"--- Min deviation from prior: %.4f\" % np.min(prior_real_diff))\n\n            elif (\n                self.rule == \"iid\"\n                and self.dataset == \"cifar100\"\n                and self.unbalanced_sgm == 0\n            ):\n                assert len(trn_y) // 100 % self.n_client == 0\n                # Only have the number clients if it divides 500\n                # Perfect IID partitions for cifar100 instead of shuffling\n                idx = np.argsort(trn_y[:, 0])\n                n_data_per_clnt = len(trn_y) // self.n_client\n                # clnt_x dtype needs to be float32, the same as weights\n                clnt_x = np.zeros(\n                    (self.n_client, n_data_per_clnt, 3, 32, 32), dtype=np.float32\n                )\n                clnt_y = np.zeros((self.n_client, n_data_per_clnt, 1), dtype=np.float32)\n                trn_x = trn_x[idx]  # 50000*3*32*32\n                trn_y = trn_y[idx]\n                n_cls_sample_per_device = n_data_per_clnt // 100\n                for i in range(self.n_client):  # devices\n                    for j in range(100):  # class\n                        clnt_x[\n                            i,\n                            n_cls_sample_per_device\n                            * j : n_cls_sample_per_device\n                            * (j + 1),\n                            :,\n                            :,\n                            :,\n                        ] = trn_x[\n                            500 * j\n                            + n_cls_sample_per_device * i : 500 * j\n                            + n_cls_sample_per_device * (i + 1),\n                            :,\n                            :,\n                            :,\n                        ]\n                        clnt_y[\n                            i,\n                            n_cls_sample_per_device\n                            * j : n_cls_sample_per_device\n                            * (j + 1),\n                            :,\n                        ] = trn_y[\n                            500 * j\n                            + n_cls_sample_per_device * i : 500 * j\n                            + n_cls_sample_per_device * (i + 1),\n                            :,\n                        ]\n\n            elif self.rule == \"iid\":\n\n                clnt_x = [\n                    np.zeros(\n                        (clnt_data_list[clnt__], self.channels, self.height, self.width)\n                    ).astype(np.float32)\n                    for clnt__ in range(self.n_client)\n                ]\n                clnt_y = [\n                    np.zeros((clnt_data_list[clnt__], 1)).astype(np.int64)\n                    for clnt__ in range(self.n_client)\n                ]\n\n                clnt_data_list_cum_sum = np.concatenate(\n                    ([0], np.cumsum(clnt_data_list))\n                )\n                for clnt_idx_ in range(self.n_client):\n                    clnt_x[clnt_idx_] = trn_x[\n                        clnt_data_list_cum_sum[clnt_idx_] : clnt_data_list_cum_sum[\n                            clnt_idx_ + 1\n                        ]\n                    ]\n                    clnt_y[clnt_idx_] = trn_y[\n                        clnt_data_list_cum_sum[clnt_idx_] : clnt_data_list_cum_sum[\n                            clnt_idx_ + 1\n                        ]\n                    ]\n\n                clnt_x = np.asarray(clnt_x)\n                clnt_y = np.asarray(clnt_y)\n\n            self.clnt_x = clnt_x\n            self.clnt_y = clnt_y\n\n            self.tst_x = tst_x\n            self.tst_y = tst_y\n\n            # Save data\n            os.mkdir(\"%s/%s\" % (self.data_path, self.name))\n\n            np.save(\"%s/%s/clnt_x.npy\" % (self.data_path, self.name), clnt_x)\n            np.save(\"%s/%s/clnt_y.npy\" % (self.data_path, self.name), clnt_y)\n\n            np.save(\"%s/%s/tst_x.npy\" % (self.data_path, self.name), tst_x)\n            np.save(\"%s/%s/tst_y.npy\" % (self.data_path, self.name), tst_y)\n\n        else:\n            print(\"Data is already downloaded in the folder.\")\n            self.clnt_x = np.load(\n                \"%s/%s/clnt_x.npy\" % (self.data_path, self.name), allow_pickle=True\n            )\n            self.clnt_y = np.load(\n                \"%s/%s/clnt_y.npy\" % (self.data_path, self.name), allow_pickle=True\n            )\n            self.n_client = len(self.clnt_x)\n\n            self.tst_x = np.load(\n                \"%s/%s/tst_x.npy\" % (self.data_path, self.name), allow_pickle=True\n            )\n            self.tst_y = np.load(\n                \"%s/%s/tst_y.npy\" % (self.data_path, self.name), allow_pickle=True\n            )\n\n            if self.dataset == \"mnist\":\n                self.channels = 1\n                self.width = 28\n                self.height = 28\n                self.n_cls = 10\n            elif self.dataset == \"cifar10\":\n                self.channels = 3\n                self.width = 32\n                self.height = 32\n                self.n_cls = 10\n            elif self.dataset == \"cifar100\":\n                self.channels = 3\n                self.width = 32\n                self.height = 32\n                self.n_cls = 100\n            elif self.dataset == \"fashion_mnist\":\n                self.channels = 1\n                self.width = 28\n                self.height = 28\n                self.n_cls = 10\n            elif self.dataset == \"emnist\":\n                self.channels = 1\n                self.width = 28\n                self.height = 28\n                self.n_cls = 10\n\n        print(\"Class frequencies:\")\n        count = 0\n        for clnt in range(self.n_client):\n            print(\n                \"Client %3d: \" % clnt\n                + \", \".join(\n                    [\n                        \"%.3f\" % np.mean(self.clnt_y[clnt] == cls)\n                        for cls in range(self.n_cls)\n                    ]\n                )\n                + \", Amount:%d\" % self.clnt_y[clnt].shape[0]\n            )\n            count += self.clnt_y[clnt].shape[0]\n\n        print(\"Total Amount:%d\" % count)\n        print(\"--------\")\n\n        print(\n            \"      Test: \"\n            + \", \".join(\n                [\"%.3f\" % np.mean(self.tst_y == cls) for cls in range(self.n_cls)]\n            )\n            + \", Amount:%d\" % self.tst_y.shape[0]\n        )\n\n\ndef generate_syn_logistic(\n    dimension,\n    n_clnt,\n    n_cls,\n    avg_data=4,\n    alpha=1.0,\n    beta=0.0,\n    theta=0.0,\n    iid_sol=False,\n    iid_dat=False,\n):\n\n    # alpha is for minimizer of each client\n    # beta  is for distirbution of points\n    # theta is for number of data points\n\n    diagonal = np.zeros(dimension)\n    for j in range(dimension):\n        diagonal[j] = np.power((j + 1), -1.2)\n    cov_x = np.diag(diagonal)\n\n    samples_per_user = (\n        np.random.lognormal(mean=np.log(avg_data + 1e-3), sigma=theta, size=n_clnt)\n    ).astype(int)\n    print(\"samples per user\")\n    print(samples_per_user)\n    print(\"sum %d\" % np.sum(samples_per_user))\n\n    num_samples = np.sum(samples_per_user)\n\n    data_x = list(range(n_clnt))\n    data_y = list(range(n_clnt))\n\n    mean_W = np.random.normal(0, alpha, n_clnt)\n    B = np.random.normal(0, beta, n_clnt)\n\n    mean_x = np.zeros((n_clnt, dimension))\n\n    if not iid_dat:  # If IID then make all 0s.\n        for i in range(n_clnt):\n            mean_x[i] = np.random.normal(B[i], 1, dimension)\n\n    sol_W = np.random.normal(mean_W[0], 1, (dimension, n_cls))\n    sol_B = np.random.normal(mean_W[0], 1, (1, n_cls))\n\n    if iid_sol:  # Then make vectors come from 0 mean distribution\n        sol_W = np.random.normal(0, 1, (dimension, n_cls))\n        sol_B = np.random.normal(0, 1, (1, n_cls))\n\n    for i in range(n_clnt):\n        if not iid_sol:\n            sol_W = np.random.normal(mean_W[i], 1, (dimension, n_cls))\n            sol_B = np.random.normal(mean_W[i], 1, (1, n_cls))\n\n        data_x[i] = np.random.multivariate_normal(mean_x[i], cov_x, samples_per_user[i])\n        data_y[i] = np.argmax((np.matmul(data_x[i], sol_W) + sol_B), axis=1).reshape(\n            -1, 1\n        )\n\n    data_x = np.asarray(data_x)\n    data_y = np.asarray(data_y)\n    return data_x, data_y\n\n\nclass DatasetSynthetic:\n    def __init__(\n        self,\n        alpha,\n        beta,\n        theta,\n        iid_sol,\n        iid_data,\n        n_dim,\n        n_clnt,\n        n_cls,\n        avg_data,\n        name_prefix,\n    ):\n        self.dataset = \"synt\"\n        self.name = name_prefix + \"_\"\n        self.name += \"%d_%d_%d_%d_%f_%f_%f_%s_%s\" % (\n            n_dim,\n            n_clnt,\n            n_cls,\n            avg_data,\n            alpha,\n            beta,\n            theta,\n            iid_sol,\n            iid_data,\n        )\n\n        data_path = \"Data\"\n        if not os.path.exists(\"%s/%s/\" % (data_path, self.name)):\n            # Generate data\n            print(\"Sythetize\")\n            data_x, data_y = generate_syn_logistic(\n                dimension=n_dim,\n                n_clnt=n_clnt,\n                n_cls=n_cls,\n                avg_data=avg_data,\n                alpha=alpha,\n                beta=beta,\n                theta=theta,\n                iid_sol=iid_sol,\n                iid_dat=iid_data,\n            )\n            os.mkdir(\"%s/%s/\" % (data_path, self.name))\n            np.save(\"%s/%s/data_x.npy\" % (data_path, self.name), data_x)\n            np.save(\"%s/%s/data_y.npy\" % (data_path, self.name), data_y)\n        else:\n            # Load data\n            print(\"Load\")\n            data_x = np.load(\n                \"%s/%s/data_x.npy\" % (data_path, self.name), allow_pickle=True\n            )\n            data_y = np.load(\n                \"%s/%s/data_y.npy\" % (data_path, self.name), allow_pickle=True\n            )\n\n        for clnt in range(n_clnt):\n            print(\n                \", \".join([\"%.4f\" % np.mean(data_y[clnt] == t) for t in range(n_cls)])\n            )\n\n        self.clnt_x = data_x\n        self.clnt_y = data_y\n\n        self.tst_x = np.concatenate(self.clnt_x, axis=0)\n        self.tst_y = np.concatenate(self.clnt_y, axis=0)\n        self.n_client = len(data_x)\n        print(self.clnt_x.shape)\n\n\nclass Dataset(torch.utils.data.Dataset):\n\n    def __init__(self, data_x, data_y=True, train=False, dataset_name=\"\"):\n        self.name = dataset_name\n        if self.name == \"mnist\" or self.name == \"synt\" or self.name == \"emnist\":\n            self.X_data = torch.tensor(data_x).float()\n            self.y_data = data_y\n            if not isinstance(data_y, bool):\n                self.y_data = torch.tensor(data_y).float()\n\n        elif self.name == \"cifar10\" or self.name == \"cifar100\":\n            self.train = train\n            self.transform = transforms.Compose([transforms.ToTensor()])\n\n            self.X_data = data_x\n            self.y_data = data_y\n            if not isinstance(data_y, bool):\n                self.y_data = data_y.astype(\"float32\")\n\n        elif self.name == \"shakespeare\":\n\n            self.X_data = data_x\n            self.y_data = data_y\n\n            self.X_data = torch.tensor(self.X_data).long()\n            if not isinstance(data_y, bool):\n                self.y_data = torch.tensor(self.y_data).float()\n\n    def __len__(self):\n        return len(self.X_data)\n\n    def __getitem__(self, idx):\n        if self.name == \"mnist\" or self.name == \"synt\" or self.name == \"emnist\":\n            X = self.X_data[idx, :]\n            if isinstance(self.y_data, bool):\n                return X\n            else:\n                y = self.y_data[idx]\n                return X, y\n\n        elif self.name == \"cifar10\" or self.name == \"cifar100\":\n            img = self.X_data[idx]\n            if self.train:\n                img = (\n                    np.flip(img, axis=2).copy() if (np.random.rand() > 0.5) else img\n                )  # Horizontal flip\n                if np.random.rand() > 0.5:\n                    # Random cropping\n                    pad = 4\n                    extended_img = np.zeros((3, 32 + pad * 2, 32 + pad * 2)).astype(\n                        np.float32\n                    )\n                    extended_img[:, pad:-pad, pad:-pad] = img\n                    dim_1, dim_2 = np.random.randint(pad * 2 + 1, size=2)\n                    img = extended_img[:, dim_1 : dim_1 + 32, dim_2 : dim_2 + 32]\n            img = np.moveaxis(img, 0, -1)\n            img = self.transform(img)\n            if isinstance(self.y_data, bool):\n                return img\n            else:\n                y = self.y_data[idx]\n                return img, y\n\n        elif self.name == \"shakespeare\":\n            x = self.X_data[idx]\n            y = self.y_data[idx]\n            return x, y\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Main","metadata":{}},{"cell_type":"code","source":"def run_experiment_for_pretrained(path_pt, tag, channel_type, args_override=None, malicious_ids=None, attack_params=None):\n    \"\"\"\n    Runs one full FedDyn experiment using a pretrained checkpoint at path_pt.\n    - tag: short string for filenames (e.g. 'feddyn_ch0')\n    - channel_type: int (0,1,2) to control channel generation behavior\n    - args_override: optional dict to override args_parser() values\n    - malicious_ids: list of client indices that are adversarial\n    - attack_params: dict passed to malicious clients (keys: poison_fraction, epsilon, poison_labels)\n    Returns: saved filepath (from save_performance)\n    \"\"\"\n    # config\n    args = args_parser()\n    if args_override:\n        for k, v in args_override.items():\n            setattr(args, k, v)\n    args.channel_type = channel_type\n\n    # Data\n    data_obj = DatasetObject(dataset=\"cifar10\", n_client=args.n_clients, rule=args.rule, unbalanced_sgm=0)\n    client_x_all = data_obj.clnt_x\n    client_y_all = data_obj.clnt_y\n    cent_x = np.concatenate(client_x_all, axis=0)\n    cent_y = np.concatenate(client_y_all, axis=0)\n\n    # weights\n    weight_list = np.asarray([len(client_y_all[i]) for i in range(args.n_clients)])\n    if args.algorithm_name in (\"FedDyn\", \"SCAFFOLD\"):\n        weight_list = weight_list / np.sum(weight_list) * args.n_clients\n\n    # device & model factory\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    model_func = lambda: Model(args.model_name)\n\n    # load pretrained checkpoint robustly\n    print(f\"\\nLoading checkpoint {path_pt} for tag {tag} -> device {device}\")\n    ckpt = torch.load(path_pt, map_location=device)\n    if isinstance(ckpt, dict) and \"state_dict\" in ckpt:\n        sd = ckpt[\"state_dict\"]\n    elif isinstance(ckpt, dict) and \"model_state_dict\" in ckpt:\n        sd = ckpt[\"model_state_dict\"]\n    elif isinstance(ckpt, dict):\n        sd = ckpt\n    else:\n        raise RuntimeError(f\"Unsupported checkpoint format: {type(ckpt)} for {path_pt}\")\n\n    # strip DataParallel prefix if present\n    sd_clean = {}\n    for k, v in sd.items():\n        new_k = k.replace(\"module.\", \"\") if isinstance(k, str) and k.startswith(\"module.\") else k\n        sd_clean[new_k] = v\n\n    init_model = model_func().to(device)\n    try:\n        init_model.load_state_dict(sd_clean)\n        print(\"Loaded pretrained weights into init_model OK.\")\n    except Exception as e:\n        print(\"Failed to load checkpoint into model:\", e)\n        # provide diagnostic\n        sample_keys = list(sd.keys())[:50]\n        print(\"Sample checkpoint keys:\", sample_keys)\n        raise\n\n    init_par_list = get_model_params([init_model])[0]\n    n_param = len(init_par_list)\n    args.n_param = n_param\n\n    np.random.seed(args.rand_seed)\n\n    # Channel setup (use args defaults and some constants)\n    fc = 915 * 10**6\n    BS_Gain = 10 ** (5.0 / 10)\n    RIS_Gain = 10 ** (5.0 / 10)\n    User_Gain = 10 ** (0.0 / 10)\n    dimen_RIS = 1.0 / 10\n    BS = np.array([-50, 0, 10])\n    RIS = np.array([0, 0, 10])\n    x0 = np.ones([args.n_clients], dtype=int)\n\n    channel = Channel(\n        SNR=args.SNR,\n        n_clients=args.n_clients,\n        location_range=args.location_range,\n        fc=fc,\n        alpha_direct=args.alpha_direct,\n        n_RIS_ele=args.n_RIS_ele,\n        n_receive_ant=args.n_receive_ant,\n        User_Gain=User_Gain,\n        x0=x0,\n        BS=BS,\n        BS_Gain=BS_Gain,\n        RIS=RIS,\n        RIS_Gain=RIS_Gain,\n        dimen_RIS=dimen_RIS,\n    )\n\n    gibbs = Gibbs(\n        n_clients=args.n_clients,\n        n_receive_ant=args.n_receive_ant,\n        n_RIS_ele=args.n_RIS_ele,\n        Jmax=args.Jmax,\n        weight_list=weight_list,\n        tau=args.tau,\n        nit=args.nit,\n        threshold=args.threshold,\n    )\n\n    air_comp = AirComp(\n        n_receive_ant=args.n_receive_ant,\n        weight_list=weight_list,\n        transmit_power=args.transmit_power,\n    )\n\n    # Algorithm / Server setup (we use FedDyn per your choice)\n    args.data_obj = data_obj\n    args.air_comp = air_comp\n    args.model_func = model_func\n    args.init_model = init_model\n    args.algorithm_name = \"FedDyn\"\n    algorithm_factory = AlgorithmFactory(args)\n    algorithm = algorithm_factory.create_algorithm(args.algorithm_name)\n\n    # malicious setup default (if user didn't pass)\n    if malicious_ids is None:\n        malicious_ids = []  # default no attackers\n    if attack_params is None:\n        attack_params = {\"poison_fraction\": 0.0, \"epsilon\": 0.0, \"poison_labels\": None}\n\n    # create clients list (each starts with init params)\n    clients_list = np.array([\n        Client(\n            algorithm=algorithm,\n            device=device,\n            weight=float(weight_list[i]),\n            train_data_X=client_x_all[i],\n            train_data_Y=client_y_all[i],\n            model=init_model,                     # template; local_train will re-init model\n            client_param=np.copy(init_par_list),\n            malicious=(i in malicious_ids),\n            attack_params=attack_params if (i in malicious_ids) else None,\n            client_id=i\n        )\n        for i in range(args.n_clients)\n    ])\n\n    # Save initial model for reproducibility\n    out_init_path = f\"Output/{data_obj.name}/{tag}_init_mdl.pt\"\n    os.makedirs(os.path.dirname(out_init_path), exist_ok=True)\n    torch.save(init_model.state_dict(), out_init_path)\n    print(\"Saved init model to\", out_init_path)\n\n    # FedDyn-specific init\n    if args.algorithm_name == \"FedDyn\":\n        local_param_list = np.zeros((args.n_clients, n_param)).astype(\"float32\")\n        cloud_model = model_func().to(device)\n        cloud_model.load_state_dict(copy.deepcopy(dict(init_model.named_parameters())))\n        cloud_model_param = get_model_params([cloud_model], n_param)[0]\n\n    # performance buffers\n    trn_perf_sel = np.zeros((args.comm_rounds, 2))\n    trn_perf_all = np.zeros((args.comm_rounds, 2))\n    tst_perf_sel = np.zeros((args.comm_rounds, 2))\n    tst_perf_all = np.zeros((args.comm_rounds, 2))\n\n    avg_model = model_func().to(device)\n    avg_model.load_state_dict(copy.deepcopy(dict(init_model.named_parameters())))\n    all_model = model_func().to(device)\n    all_model.load_state_dict(copy.deepcopy(dict(init_model.named_parameters())))\n    server = Server(avg_model, all_model, device, algorithm)\n\n    # COMMUNICATION ROUNDS\n    for t in range(args.comm_rounds):\n        print(f\"\\n=== Round {t+1}/{args.comm_rounds} (model={tag}) ===\")\n\n        # Channel/Gibbs selection\n        if not args.noiseless:\n            h_d, G, x, sigma = channel.generate()\n            if args.channel_type == 0:\n                pass\n            elif args.channel_type == 1:\n                h_d = np.zeros_like(h_d)\n            elif args.channel_type == 2:\n                h_d, G, x, sigma = generate_rician_single_ris(args, channel_seed=args.rand_seed, round_idx=t)\n            else:\n                raise ValueError(\"Unsupported channel type\")\n\n            start = time.time()\n            x_optim, f_optim, h_optim = gibbs.optimize(h_d, G, x, sigma)\n            end = time.time()\n            print(\"Gibbs time:\", end - start, \"s\")\n        else:\n            # noiseless: random selection with act_prob\n            inc_seed = 0\n            x_optim = np.array([0])\n            while np.sum(x_optim) == 0:\n                np.random.seed(t + args.rand_seed + inc_seed)\n                active_clients = np.random.uniform(size=args.n_clients)\n                x_optim = (active_clients <= args.act_prob).astype(np.int8)\n                inc_seed += 1\n            x_optim = x_optim.astype(np.int8)\n\n        selected_clnts_idx = np.where(x_optim == 1)[0]\n        selected_clnts = clients_list[selected_clnts_idx]\n        print(\"Selected indices:\", selected_clnts_idx)\n\n        # Clients local training\n        for i, client in enumerate(selected_clnts):\n            inputs = {\"curr_round\": t, \"avg_model\": server.avg_model}\n            if args.algorithm_name == \"FedDyn\":\n                inputs[\"cloud_model\"] = cloud_model\n                inputs[\"cloud_model_param\"] = cloud_model_param\n                inputs[\"cloud_model_param_tensor\"] = torch.tensor(cloud_model_param, dtype=torch.float32, device=device)\n                inputs[\"local_param\"] = local_param_list[selected_clnts_idx[i]]\n\n            # pass attack params only for malicious clients (you can add schedule logic here)\n            if client.malicious:\n                inputs[\"attack_params\"] = client.attack_params\n\n            print(f\" -> Training client {client.client_id} (malicious={client.malicious})\")\n            client.local_train(inputs)\n\n            # FedDyn update of local_param\n            if args.algorithm_name == \"FedDyn\":\n                local_param_list[selected_clnts_idx[i]] = inputs[\"local_param\"]\n\n        # Aggregation via AirComp or noiseless averaging\n        inputs_agg = {\"clients_list\": clients_list, \"selected_clnts_idx\": selected_clnts_idx, \"weight_list\": weight_list}\n        if args.algorithm_name == \"FedDyn\":\n            inputs_agg[\"local_param_list\"] = local_param_list\n            inputs_agg[\"cloud_model\"] = cloud_model\n            inputs_agg[\"cloud_model_param\"] = cloud_model_param\n\n        if not args.noiseless:\n            # gather per-client flattened params and call air_comp.transmit (which expects shape info as n_param)\n            clients_param_list = np.array([client.client_param for client in clients_list])\n            inputs_agg[\"avg_mdl_param\"] = air_comp.transmit(\n                n_param,\n                clients_param_list[selected_clnts_idx],\n                x_optim,\n                f_optim,\n                h_optim,\n                sigma,\n            )\n        else:\n            # noiseless: weighted average of selected clients\n            clients_param_list = np.array([client.client_param for client in clients_list])\n            sel = selected_clnts_idx\n            weights_sel = weight_list[sel].reshape((-1,1))\n            inputs_agg[\"avg_mdl_param\"] = np.sum(clients_param_list[sel] * weights_sel / np.sum(weights_sel), axis=0)\n\n        # Server aggregates (FedDyn.update logic inside algorithm)\n        server.aggregate(inputs_agg)\n\n        if args.algorithm_name == \"FedDyn\":\n            cloud_model = inputs_agg[\"cloud_model\"]\n            cloud_model_param = inputs_agg[\"cloud_model_param\"]\n\n        # Evaluate & log\n        evaluate_performance(\n            cent_x, cent_y,\n            data_obj.tst_x, data_obj.tst_y,\n            data_obj.dataset,\n            server.avg_model, server.all_model, device,\n            tst_perf_sel, trn_perf_sel, tst_perf_all, trn_perf_all, t\n        )\n\n    # Save results for this run\n    run_id = time.strftime(\"%Y%m%d-%H%M%S\")\n    outpath = save_performance(\n        args.comm_rounds,\n        tst_perf_all,\n        algorithm.name + f\"_{tag}\",\n        data_obj.name,\n        args.model_name,\n        args.n_clients,\n        args.noiseless,\n        args.rule,\n        channel_type=args.channel_type,\n        run_id=run_id,\n        out_dir=\"Output/experiments\"\n    )\n    print(\"Saved results to\", outpath)\n    return outpath\n\n\n# ---------- MAIN wrapper: specify your two pretrained FedDyn model files + channel types ----------\ndef main(channel_type_override=None):\n    kaggle_dir = \"/kaggle/input/fl-edge-ai-feddyn-cifar10-iid-3channels-base/data\"\n\n    # Update these to the exact filenames you have in the dataset.\n    # Each entry: tag, path, and the channel_type the model was trained with (0,1,2)\n    model_runs = [\n        {\"tag\": \"feddyn_ch0\", \"path\": os.path.join(kaggle_dir, \"feddyn_ch0.pt\"), \"channel_type\": 0},\n        {\"tag\": \"feddyn_ch2\", \"path\": os.path.join(kaggle_dir, \"feddyn_ch2.pt\"), \"channel_type\": 2},\n    ]\n\n    # Example attacker config (same for both runs here); change per-run by extending model_runs entries\n    default_malicious_ids = [2, 7, 18]   # indices of malicious clients\n    default_attack_params = {\"poison_fraction\": 0.3, \"epsilon\": 4.0 / 255.0, \"poison_labels\": None}\n\n    saved_paths = []\n    for entry in model_runs:\n        tag = entry[\"tag\"]\n        path = entry[\"path\"]\n        ch_type = entry[\"channel_type\"] if channel_type_override is None else channel_type_override\n        if not os.path.exists(path):\n            raise FileNotFoundError(f\"Model file not found: {path}\")\n        print(f\"\\nRunning model {tag} (channel_type={ch_type}) from file: {path}\")\n        out = run_experiment_for_pretrained(\n            path_pt=path,\n            tag=tag,\n            channel_type=ch_type,\n            args_override=None,\n            malicious_ids=default_malicious_ids,\n            attack_params=default_attack_params,\n        )\n        saved_paths.append((tag, out))\n\n    print(\"\\nAll runs finished. Saved files:\")\n    for tag, p in saved_paths:\n        print(tag, \"->\", p)\n    return saved_paths\n\n\n# Run when this cell executes\nif __name__ == \"__main__\":\n    saved = main(channel_type_override=None)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}